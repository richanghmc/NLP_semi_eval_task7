{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from flair.data import Sentence\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from nltk import ngrams\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from flair.embeddings import ELMoEmbeddings\n",
    "from nltk import edit_distance\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_task1(filename, max=None, mode='orig', train=True, pad=True):\n",
    "    \"\"\" method to read in data in csv format\n",
    "        Arguments:\n",
    "        filename        path to the file to read in\n",
    "        max             maximum number of words per headline, needed for test set\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    indices = []\n",
    "    tuples = []\n",
    "    with open(filename, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "        next(reader)\n",
    "        regex = re.compile('[^a-zA-Z\\d\\s\\']') # pattern to remove non-alphanumeric characters\n",
    "        max_num_words = 0\n",
    "        cnt = 0\n",
    "        for line in reader:\n",
    "            # if cnt > 100:\n",
    "            #     break\n",
    "            ids.append(line[0].strip())\n",
    "            if train:\n",
    "                labels.append(line[4].strip())\n",
    "            text = line[1]\n",
    "            start = text.find(\"<\")\n",
    "            end = text.find(\"/>\")\n",
    "            orig = text[start:end + 2]\n",
    "            edit = line[2]\n",
    "            if mode == 'edit':\n",
    "                text = text.replace(orig, edit)\n",
    "            else:\n",
    "                while \" \" in orig:\n",
    "                    white = orig.find(\" \")\n",
    "                    new_orig = orig[white+1:]\n",
    "                    text = text.replace(orig, new_orig)\n",
    "                    orig = new_orig\n",
    "            text = text.strip()\n",
    "            text = text.strip('\\n')\n",
    "            text = text.lower()\n",
    "            text = re.sub(regex, \"\", text)\n",
    "            orig = orig.strip()\n",
    "            orig = orig.lower()\n",
    "            orig = re.sub(regex, \"\", orig)\n",
    "            edit = edit.strip()\n",
    "            edit = edit.lower()\n",
    "            tup = (orig, edit)\n",
    "            tuples.append(tup)\n",
    "            edit = re.sub(regex, \"\", edit)\n",
    "            text_list = re.split('\\s+', text)\n",
    "            if mode == 'edit':\n",
    "                edit_index = text_list.index(edit)\n",
    "            else:\n",
    "                edit_index = text_list.index(orig)\n",
    "            if len(text_list) > max_num_words:\n",
    "                max_num_words = len(text_list)\n",
    "            indices.append(edit_index)\n",
    "            texts.append(text)\n",
    "            cnt = cnt + 1\n",
    "    if pad:\n",
    "        if max:\n",
    "            max_num_words = max # ignore longest essay if max was passed as argument\"\n",
    "        padded = []\n",
    "        for i in range(len(texts)):\n",
    "            t = texts[i].split()\n",
    "            t.insert(0, \"<\")\n",
    "            if len(t) > max_num_words: # truncate longer essays\n",
    "                t = t[0:max_num_words+1]\n",
    "            while len(t) <= max_num_words: # pad shorter essays\n",
    "                t.append(\">\")\n",
    "            t = \" \".join(t)\n",
    "            padded.append(t)\n",
    "        texts = padded\n",
    "    labels = np.array(labels, dtype=float)\n",
    "    if train:\n",
    "        return texts, labels, indices, ids, tuples\n",
    "    else:\n",
    "        return texts, indices, ids, tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_SVD(x, components=700):\n",
    "    svd = TruncatedSVD(n_components=components)\n",
    "    svd_x_train = svd.fit_transform(x)\n",
    "    return svd_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, idx, ids, tups = read_data_task1(\"semeval-2020-task-7-dataset/subtask-1/train.csv\", mode='orig', pad=False)\n",
    "ed_features, ed_labels, ed_idx, ed_ids, ed_tups = read_data_task1('semeval-2020-task-7-dataset/subtask-1/train.csv', mode='edit', pad=False)\n",
    "f_dev, lab_dev, ind_dev, id_dev, t_dev = read_data_task1('semeval-2020-task-7-dataset/subtask-1/dev.csv', mode='orig', pad=False)\n",
    "ed_fdev, ed_labdev, ed_inddev, ed_iddev, ed_tdev = read_data_task1('semeval-2020-task-7-dataset/subtask-1/dev.csv', mode='edit', pad=False)\n",
    "f_test, ind_test, id_test, t_test = read_data_task1('semeval-2020-task-7-dataset/subtask-1/test.csv', train=False, mode='orig', pad=False)\n",
    "ed_ftest, ed_indtest, ed_idtest, ed_ttest = read_data_task1('semeval-2020-task-7-dataset/subtask-1/test.csv', train=False, mode='edit', pad=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
